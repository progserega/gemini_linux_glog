# Работа с программным RAID
См. так же 
=>2025.01.25_mdraid.gmi Настройка LVM

А так же  
=>https://habrahabr.ru/post/102387 горячее подключение SATA-дисков.

В случае, если необходимо загружать систему с корнем на 
=>http://ru.wikipedia.org/wiki/RAID#RAID_1 RAID1


###  Создаём раздел
Важно: при создании рейда на разделе нужно помнить, что mdraid хранит свою мета-информацию вконце блочного устройства, на котором он создаётся. В результате последний раздел на диске должен заканчиваться на мегабайт раньше, чем заканчивается сам диск - иначе mdadm может спутать рейд на последнем разделе и рейд на всём диске. И даже если на всём диске нет рейда, а рейд только на последнем разделе - будет путаница.


ВЫВОД: Конец последнего раздела должен отступать от конца диска на не менее чем на 100 кБ - лучше 1 Мб



Создаём раздел на диске размером ровно 730 Гб (730*1024=747520 Мб) для того, чтобы потом не зависить от размера винта (в случае, если бы содали максимально большой раздел для нашего винта):
```
fdisk /dev/sdc
```

Более того, раздел, отдаваемый под рейд необходимо делать не до конца диска, а с отступом от конца диска на не менее 128 Кб. Это необходимо потому, что суперблок рейда создаётся в конце раздела или в конце диска, если под рейд отдаётся весь диск. Но т.к. конец всего диска является и концом же последнего раздела на этом диске, то суперблок рейда из последних разделов будет совпадать с суперблоком рейда, созданного на всём диске. При автосборке рейда при загрузке системы возникнут ошибки, т.к. система "увидит" два одинаковых рейда - с одинаковыми суперблоками.

### Клонирование разделов
Для копирования одинаковых таблиц разделов с одного диска на другие, можно выполнить команду:
```
sfdisk -d /dev/sda | sfdisk /dev/sdb
```

Взято по 
=>http://www.opennet.ru/base/sys/raid5_sync.txt.html ссылке.

В случае больших дисков с GPT, нужно использовать утилиты из пакета **gdisk** (apt-get install gdisk):
```
  sgdisk -R=/dev/sdh /dev/sda
  sgdisk -G /dev/sdh
```
Где:
* /dev/sda - исходный диск с рабочей таблицей GPT, с которого копируем GPT
* /dev/sdh - диск-получатель, на который копируем таблицу GPT
* sgdisk -G - смена UID-ов разделов на уникальные

### Диски более 2Тб
Таблица MBR имеет ограничение в 2Тб на диск. Поэтому, в случае, если используется диск размером более 2Тб - необходимо использовать таблицу разделов 
=>http://www.gentoo.org/doc/en/handbook/handbook-ia64.xml?style=printable&part=1&chap=4 GPT.
###  Очищаем ранее созданный raid-суперблок
Если насоздавать множество рейдов (в процессе экспериментирования), то при автоматической сборке рейда (на этапе загрузки) рейд может быть собран неправильно - из новых и из "старых" разделов. Чтобы исключить данный вариант развития событий необходимо очищать суперблок рейда на соответствующем блочном устройстве, если он не нужен:
```
mdadm --zero-superblock /dev/sda
```

Вообще, софтварный рейд довольно живуч, и даже если затереть суперблок вышеописанной командой, затереть начало диска с помощью dd, пересоздать таблицу разделов - всё равно mdadm будет находить старый рейд.

Тестирование обнаружения рейда на блочном устройстве выполняется командой:
```
mdadm --examine /dev/sda
```

И всё же, чтобы избавиться от старого рейда, нужно пересоздать таблицу разделов, но создать её иную, чтобы разделы начинались по другим блокам. Например создать первый раздел на 10 Мб больше или меньше.

После этого проверить, находятся ли рейды вышеуказанной командой.

###  Создаём RAID

### Создаем RAID1 из этих разделов:

```
mdadm --create /dev/md1 --metadata=0.90 --verbose -l 1 -n 2 /dev/sdc1 /dev/sdd1
```

Где:
* -l 1 - уровень рэйда (первый)
* -a - автоматически создать устройство рэйда
* -n 2 - всего в рэйде будет 2 устройства (нужно указывать конечное количество устройств, даже если сейчас задействуем только одно устройство)
*  --metadata=0.90 - версия рейда. По умолчанию mdadm создаёт версию 1.2, которая не определяется ядром linux (сборка рейда возможна только средствами mdadm после загрузки системы или с initrd-образа).

Если же на данный момент присутствует только один диск, то взамен второго нужно поставить ключевое слово missing:

```
mdadm --create /dev/md1 --metadata=0.90 --verbose -l 1 -n 2 /dev/sdc1 missing
```

Позже, когда появится второй диск, добавить его в RAID можно командой:

```
mdadm --manage --add /dev/md1 /dev/sdd1
```
или
```
mdadm -a /dev/md1 /dev/sdd1
```


### Создаём RAID5
Рейд стоит создавать только на разделах, не на полных винтах. 
Допустим у нас есть 6 дисков, из которых 5 мы хотим отдать под RAID5, а 6-й - в горячую замену.
```
mdadm --create /dev/md1 --metadata=0.90 --level=5 --raid-devices=5 /dev/sda3 /dev/sdb3 /dev/sdc3 /dev/sdd3 /dev/sde3 --spare-devices=1 /dev/sdf3
```
Где:
*  --metadata=0.90 - версия рейда. По умолчанию mdadm создаёт версию 1.2, которая не определяется ядром linux, соответственно автоопределение рейда не проходит на этапе загрузки системы и если на данном разделе находится корень - система не загрузится (сборка рейда возможна только средствами mdadm после загрузки системы или с initrd-образа).
Получаем информацию о рейде:
```
mdadm --misc --detail /dev/md0
```

Сохраняем конфигурацию:
```
mdadm --detail --scan >> /etc/mdadm.conf
```
короткая форма комманды
```
mdadm -D -s >> /etc/mdadm.conf
```



###  Получение информации о вновь созданном RAID

```
mdadm --detail /dev/md1
```
или
```
mdadm -D /dev/md1
```
Можно сохранить её для потомков в:
```
mdadm --detail /dev/md1 > /etc/raidtab
```

###  Копируем конфигураци RAID 
Перед этим можно посмотреть что там, чтобы знать что удалять, если уже там были записи, т.к. информация будет по всем RAID-устройствам в системе:
```
mdadm --detail --scan >> /etc/mdadm.conf
```

###  Сборка уже существующего рейда:
Пробуем автоматически запустить raid:
```
mdadm --assemble --scan
```
или более короткая форма
```
mdadm -A -s
```
Эта команда запустит ещё не запущенные рейды. Даже если есть всего один диск из двух.
###  raid0
Добавление диска в запущенный raid:
```
mdadm --assemble /dev/md0 /dev/sda2 /dev/sdb2
```
###  raid5
```
mdadm --assemble /dev/md1 /dev/sda3 /dev/sdb3 /dev/sdc3 /dev/sdd3 /dev/sde3 /dev/sdf3
```

###  Востановление рейда
###  Замена сбойного диска в рейде
Помечаем /dev/sdb1 как сбойный:
```
mdadm --manage --fail /dev/md0 /dev/sdb1
```
или краткая форма
```
mdadm -f /dev/md0 /dev/sdb1
```
Если проблема в кабеле подключения жесткого диска и она решена, то возрат в рейд осуществляется
```
mdadm --re-add /dev/md0 /dev/sdb1
```
Выводим его из рейда:
```
mdadm --manage --remove /dev/md0 /dev/sdb1
```
краткая форма
```
mdadm -r /dev/md0 /dev/sdb1
```
Определяем какой именно винт нужно физически заменять:
```
ls -l /dev/disk/by-id/
```
Ищем наш sdb:
```
ls -l /dev/disk/by-id/|egrep sdb$
```
Получаем:
```
lrwxrwxrwx 1 root root  9 Ноя 13 12:19 ata-WDC_WD2002FAEX-007BA0_WD-WMAWP0467603 -> ../../sdb
lrwxrwxrwx 1 root root  9 Ноя 13 12:19 scsi-SATA_WDC_WD2002FAEX-_WD-WMAWP0467603 -> ../../sdb
lrwxrwxrwx 1 root root  9 Ноя 13 12:19 wwn-0x50014ee00369c323 -> ../../sdb
```
Соответственно, разобрав сервер, нам нужно будет заменить жёсткий диск модели **WDC WD2002FAEX** с идентификатором **WMAWP0467603** (на нём будет написано). Если я не напутал :-)

После этого переразбиваем (см. выше) новый винт (возможно у него будет иное имя - не /dev/sdb1) и подключаем в рейд:
```
mdadm --manage --add /dev/md0 /dev/sdb1
```
короткая форма
```
mdadm -a /dev/md0 /dev/sdb1
```

###  Увеличение скорости пересборки рейда
Установим минимальную
скорость в 50 Мб/сек, а максимальную в 300 Мб/cек:

```
echo 50000 > /proc/sys/dev/raid/speed_limit_min
echo 300000 > /proc/sys/dev/raid/speed_limit_max
```

Посмотреть текущую скорость ресинхронизации можно в выводе команды:

```
cat /proc/mdstat
```

Взято по 
=>http://www.opennet.ru/tips/2398_raid_speed_optimization_linux_mdadm.shtml ссылке.


###  Просмотр статистики по существующим RAID-ам:
```
cat /proc/mdstat
```
в реальном времени, выход CTRL+C
```
watch cat /proc/mdstat
```

## Создание файловой системы
В случае создания ext2/3/4:
```
mkfs.ext3 -b 4096 -E stride=128,stripe-width=384
```
Где:
* -b n - байт в кластере
* -stride=n - значение из `mdadm --misc --detail /dev/md0|grep "Chunk Size"` делённое на байт в кластере. В случае Chunk Size = 512к и 4096 байт в кластере - stride=128
* -stripe-width=n - stride * количество работающих дисков. В нашем случае raid5 с пятью дисками - 128 (stride) умножаем на 4, т.к. один диск расходуется на контрольные суммы (диск горячей замены вообще не считаем, т.к. он не участвует в работе рейда).

Можно попробовать 
=>http://busybox.net/~aldot/mkfs_stride.html калькулятор.
## Горячее подключение sata
Взято 
=>https://habrahabr.ru/post/102387/ тут.

Горячее отключение:
```
echo 1 >/sys/block/sdX/device/delete
```
Горячее подключение:
```
echo "- - -" >/sys/class/scsi_host/hostX/scan
```
##  Ссылки
=>http://xgu.ru/wiki/Программный_RAID_в_Linux]]
=>http://en.gentoo-wiki.com/wiki/Software_RAID_Install Инструкция по установке RAID в Gentoo-wiki
=>https://bbs.archlinux.org/viewtopic.php?id=88876]]
=>http://busybox.net/~aldot/mkfs_stride.html]]
=>http://habrahabr.ru/blogs/linux/111036/ Оптимизация RAID6
=>https://habrahabr.ru/post/102387 Горячее подключение SATA-дисков.

