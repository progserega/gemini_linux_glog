# Работа с lvm
## Настройка LVM 
См. так же Работа с программным RAID:
=>2025.01.25_mdraid.gmi Работа с программным RAID

## Логика 

## Просмотр информации
### Краткое описание
Описание групп:
```
vgs
```
Описание логических разделов:
```
lvs
```
Описание физических разделов:
```
pvs
```

### Информация о dm-X
Чтобы узнать, что такое /dev/dm-0 или иное, нужно дать команду:
```
dmsetup info /dev/dm-0
```

### Детальное описание
Описание групп:
```
vgdisplay
```
Описание логических разделов:
```
lvdisplay
```
Описание физических разделов:
```
pvdisplay
```


## Изменение размера

Допустим нам нужно изменить логический раздел **root** в группе разделов **pve** и сделать его размером 10 Гб:
```
lvresize -r -L10G /dev/pve/root
```
Где:
* ''-r'' - это команда, предписывающая запустить соответствующие утилиты для изменения файловой системы на разделе (что очень
* удобно)
* ''-L10G'' - размер 10 Гиб (основание 10), чтобы сделать 10 Гб (основание 2), нужно указать так: ''-L10g''
* Чтобы указать использовать всё свободное место, нужно передать параметры: ''-l 100%FREE''
### Увеличение логического раздела
### Уменьшение логического раздела

## Создание LVM поверх файлов
Взято тут:
=>http://help.ubuntu.ru/wiki/lvm

Правим фильтры LVM в файле ''/etc/lvm/lvm.conf'', добавляя поиск LVM в том числе и на
=>http://help.ubuntu.ru/wiki/loop loop-устройствах:
```
filter # [ "r|/dev/nbd.*|", "a/.*/" , "a/loop/" ]
```
Создаём пустые файлы:
```
mkdir /mnt/sdc1/lvm
cd /mnt/sdc1/lvm
dd if#/dev/zero of#./d01 count#1 bs#1G
dd if#/dev/zero of#./d02 count#1 bs#1G
dd if#/dev/zero of#./d03 count#1 bs#1G
dd if#/dev/zero of#./d04 count#1 bs#1G
```
Создаем loopback устройства из файлов:
```
losetup -f --show ./d01
losetup -f --show ./d02
losetup -f --show ./d03
losetup -f --show ./d04
```
Дальше поступаем так же, как если бы ми создавали LVM на реальных дисках. Обратите внимание на названия loop-устройств —
они могут отличаться от приведённых здесь.
```
pvcreate /dev/loop0
pvcreate /dev/loop1
pvcreate /dev/loop2
```

## Бэкап с помощью снапшотов
Перед тем как вносить какие-то важные изменения в систему, например обновление - можно сделать бэкап с помощью
снапшотов:
```
sync
lvcreate -s -n root_before_update -L20G /dev/vg_root/root
```
С этого момента вся разница между текущим состоянием /dev/vg_root/root и состоянием /dev/vg_root/root на момент создания
снапшота помещаются в новый логический-диск-снапшот /dev/vg_root/root_before_update. Его можно примонтировать и брать с
него данные.

В случае, если обновление было неуспешным и необходимо откатиться на состояние на время создания снапшота, нужно
применить данный снапшот к логическому диску, с которого он был сделан:
```
lvconvert --merge /dev/vg_root/root_before_update
```
В случае, если раздел, к которому применяется снапшот в данный момент используется (например в случае root-раздела), то
снапшот применится в момент перезагрузки системы.
```
reboot
```
В случае, если система после обновления или после rm -rf /* совсем не оживает и банально нечем применять снапшот, для
отката изменений, то можно загрузиться с livecd, поднять lvm и md (если необходимо) и уже там применить снапшот.

ПРИМЕЧАНИЕ:
Применение снапшота - может быть отложенной операцией, которая продолжится после перезагрузки системы. Мониторинг применения
снапшота можно отслеживать командой:
```
dmsetup status vg_root/root_deb
```
Где:
* vg_root/root_deb - группа/логически_диск, на который накатывается снапшот

В выводе будет:
```
0 125829120 snapshot-merge 1922376/125829120 7488
```
Где:
* 7488 - сколько осталось (блоков?) для применения снапшота. Цифры уменьшаются с каждым разом.

В случае, если корень был повер программного рейда - он может начать пересобираться после перезагрузки.

Если же снапшот не пригодился и больше не нужен, то можно удалить его:
```
lvremove /dev/vg_root/root_before_update
```


## Замена физического диска/миграция данных в LVM
### Миграция данных
Если нужно убрать из логической группы используемый физический диск (например /dev/sda1), то нужно:
перенести с него данные на другой физический диск 
```
pvmove /dev/sda1
```
в данном случае данные с sda1
переместятся на другие диски, доступные в логической группе.
Примечание:
перед `pvmove` необходимо удалить все снапшоты:
```
lvremove vg_root/root_snap
```
убрать физический диск из логической группы: 
```
vgreduce vg_root /dev/sda1
```
### Миграция root-раздела
В случае, если root-каздел расположен поверх lvm, то нужно не забыть обновить конфигурации загрузки:
В случае, если lvm установлен дополнительно ещё и поверх 
=>glog/2025.01.25_mdraid.gmi RAID
то нужно
обновить конфигурацию рейда: 
```
mdadm --detail --scan >> /etc/mdadm/mdadm.conf
```
предварительно убрав
оттуда старые строки про **ARRAY**
* Обновляем initrd-образы: 
```
update-initrd -u
```
* перегенерируем конфиг:
```
dpkg-reconfigure grub-pc
```
* устанавливаем загрузкчик на диск, с которого будет грузится сервер: 
```
grub-install --recheck /dev/sda
```
Примечание:
возможно, чтобы grub удачно загрузил корень, нужно, чтобы данные корня
(как минимум ядро и initrd-образ) были на том диске, с которого грузится система. Для этого,
возможно, придётся дополнительно в этом удостовериться, мигрировав данные корня на этот диск, либо на
=>glog/2025.01.25_mdraid.gmi RAID
, если загрузочный диск является частью этого рейда.
Например:
```
pvmove -n vg_root/root /dev/md1 /dev/md0
```
Где:
* vg_root/root - логический диск LVM с корнем системы
* /dev/md1 - физическое устройство LVM (в составе группы томов vg_root), хранящее данные
* логического диска root в данный момент
* /dev/md0 - физическое устройство LVM (в составе группы томов vg_root), состоящее в том числе и
* из /dev/sda, с которого происходит загрузка системы



## Решение проблем
### Монтирование раздела образа
Взято
=>http://dark123us.wordpress.com/2011/02/08/kvm-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0-%D1%81-lvm-%D0%B4%D0%B8%D1%81%D0%BA%D0%BE%D0%BC/ тут
Смотрим расположение разделов на диске

```
# parted hda.img
GNU Parted 1.7.1
Using /data/rabbit/disk_image/test2
Welcome to GNU Parted! Type 'help' to view a list of commands.
(parted) unit
Unit?  [compact]? B
(parted) print
Disk /data/rabbit/disk_image/test2: 10262568959B
Sector size (logical/physical): 512B/512B
Partition Table: msdos
Number  Start        End           Size         Type     File system  Flags
1      32256B       106928639B    106896384B   primary  ext3         boot
2      106928640B   1184440319B   1077511680B  primary  linux-swap
3      1184440320B  10256924159B  9072483840B  primary  ext3
(parted) quit
```
Получаем адреса, где начинаются разделы, монтируем в соответствии с полученной информацией
```
mount -o loop,ro,offset#32256 hda.img /mnt/rabbit
```

При монтировании второго диска произошла ошибка
```
#umount /mnt/rabbit
#mount -o loop,ro,offset#1184440320 test2 /mnt/rabbit
#mount: wrong fs type, bad option, bad superblock on /dev/loop0,
missing codepage or helper program, or other error
In some cases useful info is found in syslog - try
dmesg | tail  or so
#dd if#hda.img of#hda3.img bs#512 skip#2313360 count#17719695
17719695+0 records in
17719695+0 records out
9072483840 bytes (9.1 GB) copied, 485.679 seconds, 18.7 MB/s
```
Поэтому решаем – копируем раздел со смещениями, указывая размер блока и сколько блоков
необходимо
Монтируем

```
# mount -o loop hda3.img /mnt/rabbit/
# df -h /mnt/rabbit
Filesystem            Size  Used Avail Use% Mounted on
/data/rabbit/image/hda3.img
8.4G  6.3G  1.7G  80% /mnt/rabbit
```


## Настройка lvm cache на nvme в том числе для root-раздела  
### Логика работы

=>https://www.it-world24.ru/programmy/lvm-cache-prozrachnoe-keshirovanie-hdd-ispolzuya-ssd.html

### Логика
* Берём быстрый диск nvme/ssd.
* Добавляем его весь в lvm-группу, в которой будем кэшировать логический раздел.
* создаём раздел кэша и метаданных кэша на этом pv
* подключаем эти разделы к кэшируемому lv
* раздел кэша может быть меньше кэшируемого lv

### Подготавливаем систему
Добавляем весь ssd в lvm-группу, которую будем кэшировать (в нашем случае vg_root):
```
lvcreate -n root_cache_metadata -L600M vg_root /dev/nvme0n1
```

## Собственно создаём lvm cache:
```
lvcreate -n root_cache_metadata -L600M vg_root /dev/nvme0n1
lvcreate -n root_cache_data -L60G vg_root /dev/nvme0n1
lvconvert --type cache-pool --cachemode writeback --poolmetadata vg_root/root_cache_metadata  vg_root/root_cache_data
lvconvert --type cache --cachepool vg_root/root_cache_data vg_root/root_deb
```

### настраиваем опции ядра:
в /etc/default/grub правим параметр опций ядра:
```
GRUB_CMDLINE_LINUX#"rd.md#1 rd.md.conf#1 rd.auto#1"
```

### Настраиваем dracut:
```
apt-get isntall dracut
```
### Добавляем нужные модули:
Создаём файл загрузки модулей /etc/dracut.conf.d/lvm.conf с содержимым (обязательно вокруг имени модуля - пробелы):
```
# support lvm root:
add_dracutmodules+#" lvm "
```
Создаём файл загрузки модулей /etc/dracut.conf.d/mdadm.conf с содержимым (обязательно вокруг имени модуля - пробелы):
```
# support mdadm root:
add_dracutmodules+=" mdraid "
```

### Пересобираем initramfs-образы для всех ядер

Смотрим какие модули есть (для каких ядер):
```
ls /lib/modules
```
Пересобираем:

```
dracut /boot/initrd.img-6.1.0-13-amd64 6.1.0-13-amd64 --force
dracut /boot/initrd.img-6.1.0-15-amd64 6.1.0-15-amd64 --force
```

## Обновляем параметры загрузки
Пересобираем конфиг с новыми параметрами ядра, прописываем новые initramfs, прописываем загрузку в блочные устройства:
```
dpkg-reconfigure grub-pc
```

### Отключение кэша

```
lvconvert --uncache vg_root/root_deb
```

## Графические утилиты для работы с LVM

* kvpm
* system-config-lvm

## Ссылки
=>http://habrahabr.ru/post/67283/ Вводная на русском
=>http://xgu.ru/wiki/LVM Документация на русском
=>http://dark123us.wordpress.com/2011/02/08/kvm-%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0-%D1%81-lvm-%D0%B4%D0%B8%D1%81%D0%BA%D0%BE%D0%BC/ Монтирование разделов образа (с помощью опции mount - offset)
=>http://help.ubuntu.ru/wiki/lvm Создание LVM поверх обычных файлов

#>../index.gmi 🔙 вернуться к началу...
